{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Drift\n",
    "\n",
    "\n",
    "Suppose the spam generator becomes more intelligent and begins producing prose which looks \"more legitimate\" than before. Data drift occurs when the data the model was trained on no longer accurately reflects the data that the model is currently analyzing. Drift can take on different forms, to illustrate a few: \n",
    " + The structure of data may change. Maybe spam emails start utilizing photo attachments rather than text. Since our model is based off of text within the email, it would likely start performing very poorly.\n",
    " + Data can change meaning, even if structure does not. Perhaps spam mail about food becomes our new favorite reading to go along with morning coffee, and we no longer want that type of spam filtered out of our mailbox.\n",
    " + Features may change. Features that are previously infrequent may become more frequent, or vice versa. One (unlikely) drift could be that all modern spam emails begin containing the word \"coffee\" and never the word \"tree.\" This could be an important insight to include in our model. \n",
    " \n",
    "Data drift appears in many subtle ways, causing models to become useless without ever notifying the user that an error has occurred. Models with changing data need to be monitored to ensure that the model is still performing as expected. \n",
    "\n",
    "We'll start exploring data drift by importing the data used in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "df = pd.read_parquet(os.path.join(\"data\", \"training.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and testing sets, as in the modelling notebooks. We use the `random_state` parameter to ensure that the data is split in the same way as it was when we fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "df_train, df_test = model_selection.train_test_split(df, random_state=43)\n",
    "df_test_spam = df_test[df_test.label == 'spam'].copy() #filter the spam documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we filter out the spam and force the spam data to drift by adding the first few lines of Pride and Prejudice to the start of the spam documents in our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(doc, adds):\n",
    "    \"\"\"\n",
    "    takes in a string _doc_ and\n",
    "    appends text _adds_ to the start\n",
    "    \"\"\"\n",
    "    \n",
    "    return adds + doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pride_pred = '''It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.“My dear Mr. Bennet,” said his lady to him one day, “have you heard that Netherfield Park is let at last?” Mr. Bennet replied that he had not. “But it is,” returned she; “for Mrs. Long has just been here, and she told me all about it.” Mr. Bennet made no answer. “Do you not want to know who has taken it?” cried his wife impatiently.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending text to the start of the spam\n",
    "df_test_spam[\"text\"] = df_test_spam.text.apply(add_text, adds=pride_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # ensures that all the text is visible\n",
    "df_test_spam.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now pass this \"drifted\" data through the pipeline we created: we compute feature vectors, and we make spam/legitimate classifications using the model we trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pickle, os\n",
    "\n",
    "# loading in feature vectors pipeline\n",
    "filename = 'feature_pipeline.sav'\n",
    "feat_pipeline = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# loading model\n",
    "filename = 'model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    ('features',feat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use our feature engineering pipeline to transform the data into feature vectors. We'll then use PCA (discussed in the [visualization](01-vectors-and-visualization.ipynb) notebook) to map these large vectors to 2 dimensions so we can view the structure of the new spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_data = feature_pipeline.fit_transform(df_train[\"text\"], df_train[\"label\"])\n",
    "ft_drifted_data = feature_pipeline.fit_transform(df_test_spam[\"text\"], df_test_spam[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "DIMENSIONS = 2\n",
    "pca = sklearn.decomposition.TruncatedSVD(DIMENSIONS)\n",
    "\n",
    "# fit_transform original data, put into data frame\n",
    "pca_a = pca.fit_transform(ft_train_data)\n",
    "pca_df = pd.DataFrame(pca_a, columns=[\"x\", \"y\"])\n",
    "\n",
    "# transform new spam data, put into data frame\n",
    "pca_b = pca.transform(ft_drifted_data)\n",
    "pca_df_drift = pd.DataFrame(pca_b, columns=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair.expr import datum\n",
    "alt.renderers.enable('notebook')\n",
    "SAMPLE = 2000\n",
    "plot_data = pca_df.assign(label = df_train[\"label\"])\n",
    "plot_data[\"label\"]= plot_data[\"label\"].replace(\"spam\", \"previous spam\")\n",
    "\n",
    "plot_data_drift = pca_df_drift.assign(label = \"drifted spam\")\n",
    "\n",
    "plot_data2 = pd.concat([plot_data_drift, plot_data])\n",
    "domain = ['legitimate', 'previous spam', 'drifted spam']\n",
    "range_ = ['lightgray', 'blue', 'red']\n",
    "\n",
    "chart1 = alt.Chart(plot_data2.sample(SAMPLE))\\\n",
    "            .mark_point(opacity=0.4) \\\n",
    "            .encode(x='x', y='y', color = alt.Color('label', scale= alt.Scale(domain=domain, range=range_)))\\\n",
    "            .interactive()\n",
    "\n",
    "chart1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing drifted spam emails in red and previous spam emails in blue, it looks like structure of spam has changed drastically. There's a good chance our model no longer performs as well as it used to. Utilizing pipelines, let's make predictions for the drifted spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features',feat_pipeline),\n",
    "    ('model',model)\n",
    "])\n",
    "\n",
    "pipeline.fit(df_train[\"text\"], df_train[\"label\"])\n",
    "\n",
    "# predict test instances\n",
    "y_preds = pipeline.predict(df_test_spam[\"text\"])\n",
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though the drifted data is mostly classified as legitimate (even though the entire test set was made of spam emails), but let's look at a confusion matrix to visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from mlworkflows import plot\n",
    "\n",
    "df, chart = plot.binary_confusion_matrix(df_test_spam[\"label\"], y_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the model is quite terrible at classifying drifted data, since these spam emails look very different than the spam emails we originally trained the model with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this exploration, we've been able to see that some change in the underlying data caused our model to be no longer useful. Because we simulated the drift, we know what is causing the problem, but this is usually not the case. Further exploration may be needed: is the drift gradual or abrupt? Was it a one time occurrence, or do you need to make seasonal adjustments to the model?\n",
    "\n",
    "We'll build a more formal test to check for drift using the [Alibi Detect](https://github.com/SeldonIO/alibi-detect) library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# change to numpy arrays in order to interact with KSDrift\n",
    "array_test = np.asarray(df_test)\n",
    "array_test_spam = np.asarray(df_test_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many methods of detection, we will display [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test), or K-S, tests in this notebook to check for drift. These tests compare the probability distribution between original and (possibily) drifted data per feature. Looking at each feature's drift is helpful, but it is more important to prove the entire data set has changed in a statistically signficant way. Using a [Bonferroni](https://mathworld.wolfram.com/BonferroniCorrection.html) correction, the K-S test results are aggregated and tested as a whole. \n",
    "\n",
    "K-S tests are useful as they can detect imperceptible but statistically significant drift. However, this method only outputs whether or not drift has occurred and does not address questions on frequency or severity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KSDrift\n",
    "import alibi_detect\n",
    "from alibi_detect.cd import KSDrift\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# initialize label encoder\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "p_val = 0.05\n",
    "drift_detect = KSDrift(\n",
    "    p_val = p_val, # p-value for KS test\n",
    "    X_ref = array_test, # test against original test set\n",
    "    preprocess_fn = pca, # other options: auto-encoder, softmax output\n",
    "    preprocess_kwargs = {'model': label_encoder.fit(array_test[:,1]), 'batch_size':32},\n",
    "    alternative = 'two-sided',  # other options: 'less', 'greater'\n",
    "    correction = 'bonferroni' # other option: false discovery rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a sanity check and test the original data. Since we're feeding in the same data set twice, we should not get any drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = drift_detect.predict(array_test)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Has the data drifted? {}'.format(labels[preds_test['data']['is_drift']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the desired output! Let's try again, but with the drifted data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = drift_detect.predict(array_test_spam)\n",
    "print('Has the data drifted? {}'.format(labels[preds_test['data']['is_drift']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our drift detector can confirm that the data has drifted. Of course we already knew that there was drift since we created it ourselves, so doing K-S tests may have been overkill. However, this is a useful test when it isn't known if data has drifted or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can both visualize and prove our data has drifted. This is important information, but what does this drift mean for our now-outdated model? *There is no one-size-fits-all answer to this question.* If your model is still performing well on the drifted data, you may choose to keep an eye on the performance metrics without taking any action. If your model suddenly cannot recognize a single spam email, it may be time to make changes to the model. Updates can look different; you may choose to: \n",
    " - Retrain your model including the new data\n",
    " - Test new parameters for a better fit\n",
    " - Build a new model that suits the drifted data better\n",
    " \n",
    "or some combination of these techniques. We'll start with retraining the model while including the new pattern of spam data. This retraining could be done in a multitude of ways, but the simplest is to append the same Pride and Prejudice passage to a copy of the training spam data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append pride + prejudice to spam train \n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "# filter out spam training data\n",
    "df_train_spam_drift = df_train[df_train.label == 'spam'].copy()\n",
    "\n",
    "# add text to the start of the spam\n",
    "df_train_spam_drift[\"text\"] = df_train_spam_drift.text.apply(add_text, adds=pride_pred)\n",
    "df_train_spam_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append drifted spam data to df_train\n",
    "df_train = df_train.append(df_train_spam_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a new dataset that should capture the same type of drift. Next, let's retrain the model and look at the results. While we wouldn't normally use accurary score (Can't remember why? Look at [this notebook.](./02-evaluating-models.ipynb)), because all of our test data is spam, there are no false positives or negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain model including drifted spam\n",
    "pipeline.fit(df_train[\"text\"], df_train[\"label\"])\n",
    "\n",
    "# predict test instances\n",
    "y_preds = pipeline.predict(df_test_spam[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df_test_spam[\"label\"], y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new and improved model is much more successful at identifying the drifted emails as spam! While most models won't be 100% accurate, the drift was significant and the training data reflected an identical drift. If the results were poor, our next step might have been to adjust the parameters set in previous notebooks or research a completely new model better suited for the new data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to put streamed data into piplines in order to automatically alert users when drift occurs and retrain the model. We look at integration services in [another notebook](07-services.ipynb) to better understand other capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "The two models perform very similarly on the \"drifted\" data in this notebook. Consider alternative types of data drift and see how the models perform: \n",
    "1. What happens when fewer words from Pride and Prejudice are appended to the spam? \n",
    "2. How about using a completely different excerpt of Austen? \n",
    "3. How do the models perform when generic text (neither Austen nor food reviews) is appended to the spam? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
